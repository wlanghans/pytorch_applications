{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44d3c28d",
   "metadata": {},
   "source": [
    "# State estimation using an Extended Kalman Filter\n",
    "This notebook uses PyTorch's autograd to simplify the computation of the Jacobian needed in the Extended Kalman Filter. States will be estimated for a dynamical system, the non-linear coupled set defined by Lorenz's equations, given a number of noisy observations. \n",
    "\n",
    "## Lorenz equations and numerical discretization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5005973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def lorenz_torch(state: torch.Tensor, sigma: torch.Tensor, rho: torch.Tensor, beta: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the Lorenz system derivatives.\n",
    "    \n",
    "    Args:\n",
    "        state: tensor of shape (3,) or (..., 3), representing (x, y, z)\n",
    "        sigma: torch tensor scalar (can require gradients)\n",
    "        rho: torch tensor scalar (can require gradients)\n",
    "        beta: torch tensor scalar (can require gradients)\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: derivatives of same shape as state\n",
    "    \"\"\"\n",
    "    x, y, z = state[..., 0], state[..., 1], state[..., 2]\n",
    "\n",
    "    dx = sigma * (y - x)\n",
    "    dy = x * (rho - z) - y\n",
    "    dz = x * y - beta * z\n",
    "\n",
    "    return torch.stack([dx, dy, dz], dim=-1)\n",
    "\n",
    "def euler_step_torch(state: torch.Tensor, dt: float, sigma: torch.Tensor, rho: torch.Tensor, beta: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Perform one Euler integration step.\n",
    "    \n",
    "    Args:\n",
    "        state: torch tensor of shape (3,) or (..., 3)\n",
    "        dt: float, time step size\n",
    "        sigma: torch tensor scalar\n",
    "        rho: torch tensor scalar\n",
    "        beta: torch tensor scalar\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: updated state\n",
    "    \"\"\"\n",
    "    return state + dt * lorenz_torch(state, sigma, rho, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5dbbc8",
   "metadata": {},
   "source": [
    "Let's write a function that can advance an initial state for a certain number of time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479acc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lorenz(state0, t0, steps, dt, sigma, rho, beta):\n",
    "    state = state0\n",
    "    trajectory = []\n",
    "    time = []\n",
    "\n",
    "    for i in range(steps):\n",
    "        state = euler_step_torch(state, dt, sigma, rho, beta)\n",
    "        trajectory.append(state)\n",
    "        time.append(torch.tensor(t0 + (i + 1) * dt))\n",
    "\n",
    "    return torch.stack(time), torch.stack(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0d7f69",
   "metadata": {},
   "source": [
    "## Extended Kalman Filter description\n",
    "\n",
    "The **Extended Kalman Filter (EKF)** is a nonlinear extension of the standard Kalman Filter. It estimates the state of a nonlinear dynamical system given noisy observations.\n",
    "\n",
    "### State-Space Formulation\n",
    "\n",
    "The system is modeled as:\n",
    "\n",
    "**Process model (system dynamics):**\n",
    "$$\\mathbf{x}_{k} = f(\\mathbf{x}_{k-1}) + \\mathbf{w}_{k-1}$$\n",
    "\n",
    "**Observation model (measurement):**\n",
    "$$\\mathbf{z}_{k} = h(\\mathbf{x}_{k}) + \\mathbf{v}_{k}$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{x}_k$ is the state at time step $k$\n",
    "- $\\mathbf{z}_k$ is the observation/measurement at time $k$\n",
    "- $f(\\cdot)$ is the (possibly nonlinear) process function\n",
    "- $h(\\cdot)$ is the (possibly nonlinear) measurement function\n",
    "- $\\mathbf{w}_k \\sim \\mathcal{N}(0, \\mathbf{Q})$ is the process noise\n",
    "- $\\mathbf{v}_k \\sim \\mathcal{N}(0, \\mathbf{R})$ is the measurement noise\n",
    "- $\\mathbf{Q}$ is the process covariance matrix\n",
    "- $\\mathbf{R}$ is the measurement covariance matrix\n",
    "\n",
    "### EKF Algorithm\n",
    "\n",
    "**Prediction step:**\n",
    "$$\\hat{\\mathbf{x}}_{k|k-1} = f(\\hat{\\mathbf{x}}_{k-1|k-1})$$\n",
    "$$\\mathbf{P}_{k|k-1} = \\mathbf{F}_k \\mathbf{P}_{k-1|k-1} \\mathbf{F}_k^T + \\mathbf{Q}$$\n",
    "\n",
    "**Update step:**\n",
    "$$\\mathbf{y}_k = \\mathbf{z}_k - h(\\hat{\\mathbf{x}}_{k|k-1})$$\n",
    "$$\\mathbf{S}_k = \\mathbf{H}_k \\mathbf{P}_{k|k-1} \\mathbf{H}_k^T + \\mathbf{R}$$\n",
    "$$\\mathbf{K}_k = \\mathbf{P}_{k|k-1} \\mathbf{H}_k^T \\mathbf{S}_k^{-1}$$\n",
    "$$\\hat{\\mathbf{x}}_{k|k} = \\hat{\\mathbf{x}}_{k|k-1} + \\mathbf{K}_k \\mathbf{y}_k$$\n",
    "$$\\mathbf{P}_{k|k} = (\\mathbf{I} - \\mathbf{K}_k \\mathbf{H}_k) \\mathbf{P}_{k|k-1}$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{F}_k = \\frac{\\partial f}{\\partial \\mathbf{x}}|_{\\hat{\\mathbf{x}}_{k-1|k-1}}$ is the Jacobian of the process function\n",
    "- $\\mathbf{H}_k = \\frac{\\partial h}{\\partial \\mathbf{x}}|_{\\hat{\\mathbf{x}}_{k|k-1}}$ is the Jacobian of the measurement function\n",
    "- $\\hat{\\mathbf{x}}_{k|k}$ is the estimated state after observation\n",
    "- $\\mathbf{P}_{k|k}$ is the state covariance after observation\n",
    "- $\\mathbf{K}_k$ is the Kalman gain\n",
    "- $\\mathbf{y}_k$ is the innovation (measurement residual)\n",
    "- $\\mathbf{S}_k$ is the innovation covariance\n",
    "\n",
    "The intention in this notebook is to obtain the Jacobians through PyTorch's autograd capabilities.\n",
    "\n",
    "Let's start by defining the measurement function $h(\\mathbf{x}_{k})$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ac5292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hx(state):\n",
    "    \"\"\"\n",
    "    Measurement function: observation = state (identity mapping)\n",
    "    state: tensor of shape (3,) or (..., 3)\n",
    "    returns: observation tensor of same shape\n",
    "    \"\"\"\n",
    "   \n",
    "    return state.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d315bc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_observations(state0, noise_std=0.5, thin_factor=1):\n",
    "    # Fake \"observed\" data with measurement noise\n",
    "    with torch.no_grad():\n",
    "        true_time, true_traj = run_lorenz(\n",
    "            state0,\n",
    "            t0=0,\n",
    "            steps=100,\n",
    "            dt=0.01,\n",
    "            sigma=torch.tensor(10.0),\n",
    "            rho=torch.tensor(28.0),\n",
    "            beta=torch.tensor(8/3),\n",
    "        )\n",
    "        # Add Gaussian noise to observations\n",
    "        noise = torch.randn_like(true_traj) * noise_std\n",
    "        noisy_traj = hx(true_traj) + noise\n",
    "        return true_time[::thin_factor], noisy_traj[::thin_factor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7a5381",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Extended Kalman Filter Implementation\n",
    "\n",
    "Here we implement the EKF using PyTorch's autograd to compute Jacobians automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37e71a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtendedKalmanFilter:\n",
    "    def __init__(self, sigma, rho, beta, dt=0.01, Q=None, R=None):\n",
    "        \"\"\"\n",
    "        Initialize the Extended Kalman Filter\n",
    "        \n",
    "        Args:\n",
    "            sigma, rho, beta: Lorenz parameters\n",
    "            dt: time step\n",
    "            Q: process noise covariance (default: 1e-4 * I)\n",
    "            R: measurement noise covariance (default: 0.5^2 * I)\n",
    "        \"\"\"\n",
    "        self.sigma = sigma\n",
    "        self.rho = rho\n",
    "        self.beta = beta\n",
    "        self.dt = dt\n",
    "        \n",
    "        # Default noise covariances\n",
    "        self.Q = Q if Q is not None else 1e-4 * torch.eye(3)\n",
    "        self.R = R if R is not None else (0.5**2) * torch.eye(3)\n",
    "        \n",
    "        # State estimate and covariance\n",
    "        self.x = None\n",
    "        self.P = None\n",
    "\n",
    "        self.time = 0.0\n",
    "        \n",
    "    def process_function(self, state):\n",
    "        \"\"\"\n",
    "        Process model: x_k = f(x_{k-1})\n",
    "        One Euler step of Lorenz dynamics\n",
    "        \"\"\"\n",
    "        return euler_step_torch(state, self.dt, self.sigma, self.rho, self.beta)\n",
    "    \n",
    "    def process_with_jacobian(self, state):\n",
    "        \"\"\"\n",
    "        Compute Jacobian of process function using autograd\n",
    "        F = ∂f/∂x evaluated at state\n",
    "        \"\"\"\n",
    "        state_copy = state.clone().detach().requires_grad_(True)\n",
    "        output = self.process_function(state_copy)\n",
    "        \n",
    "        # Compute Jacobian\n",
    "        jacobian = torch.zeros(3, 3)\n",
    "        for i in range(3):\n",
    "            grad_output = torch.zeros_like(output)\n",
    "            grad_output[i] = 1.0\n",
    "            \n",
    "            if state_copy.grad is not None:\n",
    "                state_copy.grad.zero_()\n",
    "            \n",
    "            output.backward(grad_output, retain_graph=(i < 2))\n",
    "            jacobian[i, :] = state_copy.grad.clone()\n",
    "        \n",
    "        return output, jacobian\n",
    "\n",
    "\n",
    "    def measurement_with_jacobian(self, state):\n",
    "        \"\"\"\n",
    "        Compute both the three measurements (one for each state) and its Jacobian in one call\n",
    "        Returns: (z_pred, H)\n",
    "        \"\"\"\n",
    "        state_copy = state.clone().detach().requires_grad_(True)\n",
    "        z_pred = self.measurement_function(state_copy)\n",
    "        \n",
    "        # Compute Jacobian H\n",
    "        H = torch.zeros(3, 3)\n",
    "        for i in range(3):\n",
    "            grad_output = torch.zeros_like(z_pred)\n",
    "            grad_output[i] = 1.0\n",
    "            \n",
    "            if state_copy.grad is not None:\n",
    "                state_copy.grad.zero_()\n",
    "            \n",
    "            z_pred.backward(grad_output, retain_graph=(i < 2))\n",
    "            H[i, :] = state_copy.grad.clone()\n",
    "        \n",
    "        # Get the actual measurement value (without gradients)\n",
    "        z_pred = z_pred.detach()\n",
    "        \n",
    "        return z_pred, H\n",
    "\n",
    "    def measurement_function(self, state):\n",
    "        \"\"\"\n",
    "        Measurement model: z_k = h(x_k)\n",
    "        Identity mapping (we observe the full state)\n",
    "        \"\"\"\n",
    "        return hx(state)\n",
    "    \n",
    "    def predict(self, time):\n",
    "        \"\"\"\n",
    "        Prediction step of EKF\n",
    "        x_{k|k-1} = f(x_{k-1|k-1})\n",
    "        P_{k|k-1} = F_k * P_{k-1|k-1} * F_k^T + Q\n",
    "        \"\"\"\n",
    "        while self.time + self.dt <= time:\n",
    "            # Predict state\n",
    "            self.x, F = self.process_with_jacobian(self.x.detach())\n",
    "            self.time += self.dt\n",
    "            \n",
    "            # Predict covariance\n",
    "            self.P = F @ self.P @ F.T + self.Q\n",
    "        assert abs(time - self.time) < 1e-8, \"Prediction time step mismatch\"\n",
    "    \n",
    "    def update(self, z):\n",
    "        \"\"\"\n",
    "        Update step of EKF given measurement z\n",
    "        \"\"\"\n",
    "        # Compute predicted measurement and Jacobian H\n",
    "        z_pred, H = self.measurement_with_jacobian(self.x)\n",
    "        # Innovation (measurement residual)\n",
    "        y = z - z_pred\n",
    "        \n",
    "        # Innovation covariance\n",
    "        S = H @ self.P @ H.T + self.R\n",
    "        \n",
    "        # Kalman gain\n",
    "        K = self.P @ H.T @ torch.linalg.inv(S)\n",
    "        \n",
    "        # Update state estimate\n",
    "        self.x = self.x + K @ y\n",
    "        \n",
    "        # Update covariance estimate\n",
    "        self.P = (torch.eye(3) - K @ H) @ self.P\n",
    "        \n",
    "        return y.norm().item()  # Return innovation magnitude\n",
    "    \n",
    "    def initialize(self, time, x0, P0=None):\n",
    "        \"\"\"\n",
    "        Initialize filter with initial state and covariance\n",
    "        \"\"\"\n",
    "        self.time = time\n",
    "        self.x = x0.clone().detach()\n",
    "        self.P = P0 if P0 is not None else torch.eye(3) * 1.0  # Initial uncertainty\n",
    "    \n",
    "    def filter(self, observations, times):\n",
    "        \"\"\"\n",
    "        Run the EKF on a sequence of observations\n",
    "        \"\"\"\n",
    "        estimates = []\n",
    "        covariances = []\n",
    "        innovations = []\n",
    "        \n",
    "        for i, (t, z) in enumerate(zip(times, observations)):\n",
    "            self.predict(t)\n",
    "            innov = self.update(z.clone().detach())\n",
    "            \n",
    "            estimates.append(self.x.clone().detach())\n",
    "            covariances.append(self.P.clone().detach())\n",
    "            innovations.append(innov)\n",
    "        \n",
    "        return torch.stack(estimates), covariances, innovations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fd45fe",
   "metadata": {},
   "source": [
    "Now let's test this EKF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c319e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "state0 = torch.tensor([1.0, 1.0, 1.0])\n",
    "obs_times, obs_states = get_observations(state0, noise_std=2., thin_factor=4)\n",
    "\n",
    "# Create and initialize EKF\n",
    "ekf = ExtendedKalmanFilter(\n",
    "    sigma=torch.tensor(10.0),\n",
    "    rho=torch.tensor(28.0),\n",
    "    beta=torch.tensor(8/3),\n",
    "    dt=0.01,\n",
    "    Q=0.2**2 * torch.eye(3),\n",
    "    R=2.**2 * torch.eye(3)\n",
    ")\n",
    "\n",
    "ekf.initialize(time=0, x0=state0, P0=torch.eye(3) * 0.5)\n",
    "\n",
    "# Run filter\n",
    "estimated_states, cov_history, innovations = ekf.filter(obs_states, obs_times)\n",
    "\n",
    "print(f\"EKF filtering complete\")\n",
    "print(f\"Mean innovation magnitude: {np.mean(innovations):.6f}\")\n",
    "print(f\"Final estimated state: {estimated_states[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31467d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the filtered trajectory\n",
    "fig = plt.figure(figsize=(12, 9))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot the filtered trajectory\n",
    "estimated_states_np = estimated_states.numpy()\n",
    "ax.plot(estimated_states_np[:, 0], estimated_states_np[:, 1], estimated_states_np[:, 2], \n",
    "        'b-', linewidth=2, label='Filtered Estimate')\n",
    "\n",
    "# Plot the observations as scatter points\n",
    "obs_states_np = obs_states.numpy()\n",
    "ax.scatter(obs_states_np[:, 0], obs_states_np[:, 1], obs_states_np[:, 2], \n",
    "          c='red', marker='o', s=50, alpha=0.6, label='Noisy Observations')\n",
    "\n",
    "# Optional: plot the model for comparison\n",
    "with torch.no_grad():\n",
    "    _, true_traj = run_lorenz(\n",
    "        state0,\n",
    "        t0=0,\n",
    "        steps=100,\n",
    "        dt=0.01,\n",
    "        sigma=torch.tensor(10.0),\n",
    "        rho=torch.tensor(28.0),\n",
    "        beta=torch.tensor(8/3),\n",
    "    )\n",
    "true_traj_np = true_traj.numpy()\n",
    "ax.plot(true_traj_np[:, 0], true_traj_np[:, 1], true_traj_np[:, 2], \n",
    "        'g--', linewidth=1.5, alpha=0.7, label='Model trajectory')\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.set_title('Extended Kalman Filter: State Estimation')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
