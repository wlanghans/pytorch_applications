{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44d3c28d",
   "metadata": {},
   "source": [
    "# Optimizing the parameters of the Lorenz system\n",
    "This notebook explores PyTorch's autograd to optimize Lorenz's nonlinear dynamical problem. A three-dimensional state vector evolves nonlinearly in this coupled system and the solution depends on three parameters. The notebook covers and compares several different approaches to tune the parameters: optimizing the parameters using gradient-based optimization based on a single scenario, and \"online\" optimization of the parameters for that same scenario.\n",
    "\n",
    "## Lorenz equations and numerical discretization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5005973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def lorenz_torch(state: torch.Tensor, sigma: torch.Tensor, rho: torch.Tensor, beta: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute the Lorenz system derivatives.\n",
    "    \n",
    "    Args:\n",
    "        state: tensor of shape (3,) or (..., 3), representing (x, y, z)\n",
    "        sigma: torch tensor scalar (can require gradients)\n",
    "        rho: torch tensor scalar (can require gradients)\n",
    "        beta: torch tensor scalar (can require gradients)\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: derivatives of same shape as state\n",
    "    \"\"\"\n",
    "    x, y, z = state[..., 0], state[..., 1], state[..., 2]\n",
    "\n",
    "    dx = sigma * (y - x)\n",
    "    dy = x * (rho - z) - y\n",
    "    dz = x * y - beta * z\n",
    "\n",
    "    return torch.stack([dx, dy, dz], dim=-1)\n",
    "\n",
    "def euler_step_torch(state: torch.Tensor, dt: float, sigma: torch.Tensor, rho: torch.Tensor, beta: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Perform one Euler integration step.\n",
    "    \n",
    "    Args:\n",
    "        state: torch tensor of shape (3,) or (..., 3)\n",
    "        dt: float, time step size\n",
    "        sigma: torch tensor scalar\n",
    "        rho: torch tensor scalar\n",
    "        beta: torch tensor scalar\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: updated state\n",
    "    \"\"\"\n",
    "    return state + dt * lorenz_torch(state, sigma, rho, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5dbbc8",
   "metadata": {},
   "source": [
    "Let's write a function that can advance an initial state for a certain number of time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479acc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lorenz(state0, t0, steps, dt, sigma, rho, beta):\n",
    "    state = state0\n",
    "    trajectory = []\n",
    "    time = []\n",
    "\n",
    "    for i in range(steps):\n",
    "        state = euler_step_torch(state, dt, sigma, rho, beta)\n",
    "        trajectory.append(state)\n",
    "        time.append(torch.tensor(t0 + (i + 1) * dt))\n",
    "\n",
    "    return torch.stack(time), torch.stack(trajectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0d7f69",
   "metadata": {},
   "source": [
    "## Gradient-based optimization of a past scenario\n",
    "Let's now run a gradient-based optimization for one scenario. That is, let's define the \"true\" trajectory for one initial state and let's iterate over many parameter configurations (i.e., trials) to minimize the distance to the true trajectory (i.e., loss).\n",
    "\n",
    "The approach is to define the parameters first with initial guesses and to set `requires_grad=True` such that the computational graph will be computed to enable subsequent gradient-based incremental changes to the parameter set. \n",
    "\n",
    "The function below takes in the number of total time steps to use to solve the optimization. Each trajectory is currently covered by 100 time steps, such that a total of 10000 time steps would equate to 100 trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0600aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scenario_optimization(state0, num_tot_steps=10000):\n",
    "    # Set number of trials \n",
    "    num_trials = int(num_tot_steps / 100)\n",
    "\n",
    "\n",
    "    # Initial parameter choices\n",
    "    sigma = torch.tensor(8.0, requires_grad=True)\n",
    "    rho   = torch.tensor(20.0, requires_grad=True)\n",
    "    beta  = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "    true_time, true_traj = get_observations(state0)\n",
    "\n",
    "    optimizer = torch.optim.Adam([sigma, rho, beta], lr=1e-2)\n",
    "\n",
    "    # Store trajectories and losses for visualization\n",
    "    stored_trajectories = []\n",
    "    stored_losses = []\n",
    "\n",
    "    for trial in range(num_trials):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred_time, pred_traj = run_lorenz(state0, t0=0, steps=100, dt=0.01, sigma=sigma, rho=rho, beta=beta)\n",
    "        loss = torch.mean((pred_traj - true_traj) ** 2)\n",
    "\n",
    "        loss.backward()   # <-- AUTOGRAD HERE\n",
    "        optimizer.step()\n",
    "\n",
    "        if trial % 20 == 0 or trial == num_trials - 1:\n",
    "            print(f\"Trial {trial:4d} | Loss={loss.item():.6f} \"\n",
    "                f\"sigma={sigma.item():.3f}, rho={rho.item():.3f}, beta={beta.item():.3f}\")\n",
    "            # Store trajectory and loss for plotting\n",
    "            stored_trajectories.append((trial, pred_traj.detach().clone()))\n",
    "            stored_losses.append((trial, loss.item()))\n",
    "    return [traj for _, traj in stored_trajectories], [loss for _, loss in stored_losses]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9ef46d",
   "metadata": {},
   "source": [
    "Now let's run this from an initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5c23ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "state0 = torch.tensor([1.0, 1.0, 1.0])\n",
    "trajectories, losses = scenario_optimization(state0, num_tot_steps=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2922eebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trajectories\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "# 3D plot of all trajectories\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "\n",
    "# Plot the true (observed) trajectory\n",
    "true_traj_np = true_traj.detach().numpy()\n",
    "ax1.plot(true_traj_np[:, 0], true_traj_np[:, 1], true_traj_np[:, 2], \n",
    "         'o-', linewidth=2, markersize=4, label='Observed', color='black')\n",
    "\n",
    "# Plot stored trajectories with color gradient representing iterations\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(stored_trajectories)))\n",
    "for i, trial_traj in enumerate(stored_trajectories):\n",
    "    trial, traj = trial_traj\n",
    "    traj_np = traj.numpy()\n",
    "    ax1.plot(traj_np[:, 0], traj_np[:, 1], traj_np[:, 2], \n",
    "             alpha=0.3, linewidth=0.5, color=colors[i])\n",
    "\n",
    "ax1.set_xlabel('X')\n",
    "ax1.set_ylabel('Y')\n",
    "ax1.set_zlabel('Z')\n",
    "ax1.set_title('Lorenz Trajectories Over Optimization')\n",
    "ax1.legend()\n",
    "\n",
    "# Loss curve\n",
    "ax2 = fig.add_subplot(122)\n",
    "trials, losses = zip(*stored_losses)\n",
    "ax2.plot(trials, losses, 'b-', marker='o', markersize=4)\n",
    "ax2.set_xlabel('Trial')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('Loss During Optimization')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTotal trajectories plotted: {len(stored_trajectories)} (plus the observed one)\")\n",
    "print(f\"Final loss: {losses[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a728a5f3",
   "metadata": {},
   "source": [
    "## Online parameter estimation in real time\n",
    "This works fine. But one problem is that we can carry out this type of optimization only if we have observations for the full trajectory, which for some systems might only be available for past dates. That is, the optimization can only occur for past scenarios. An alternative tested next is to assume that observations trickle in in real-time. In that case, we can adjust the parameters in real-time as we move forward to arrive at an optimal parameter configuration over time. In this sense, we use the parameters to nudge the system toward the observations over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8571865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset parameters for online estimation\n",
    "sigma_online = torch.tensor(8.0, requires_grad=True)\n",
    "rho_online   = torch.tensor(20.0, requires_grad=True)\n",
    "beta_online  = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "# Starting state\n",
    "current_state = state0.clone()\n",
    "current_time = 0.0\n",
    "\n",
    "# Extract times and states from true trajectory\n",
    "true_times = true_time\n",
    "true_states = true_traj\n",
    "\n",
    "# Store results for visualization\n",
    "online_trajectories = []\n",
    "online_losses = []\n",
    "online_params = []\n",
    "\n",
    "# Number of optimization trials per observation\n",
    "# In the previous optimization we ran 2000 trials, each 100 time steps\n",
    "# Let's keep the total number of simulated steps the same such that per \n",
    "# time step we optimize with n_trials_online trials\n",
    "n_trials_online = int(25000 / 100)\n",
    "\n",
    "# Process each observation\n",
    "for obs_idx in range(len(true_states)):\n",
    "    target_time = true_times[obs_idx].item()\n",
    "    dt_to_target = target_time - current_time\n",
    "    num_steps_to_target = 1 #int(dt_to_target / 0.01)  # Currently one step from previous observation to next\n",
    "    observed_state = true_states[obs_idx]\n",
    "    target_time = num_steps_to_target * 0.01 + current_time\n",
    "    # Optimizer for this online step\n",
    "    optimizer_online = torch.optim.Adam([sigma_online, rho_online, beta_online], lr=1e-2)\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    best_pred_state = None\n",
    "    \n",
    "    # Run N optimization trials for this observation\n",
    "    for trial in range(n_trials_online):\n",
    "        optimizer_online.zero_grad()\n",
    "        \n",
    "        # Advance from current state to next observation point\n",
    "        next_state = run_lorenz(current_state, current_time, num_steps_to_target, 0.01, sigma_online, rho_online, beta_online)[1][-1]\n",
    "        \n",
    "        # Compute loss against observed state\n",
    "        loss = torch.mean((next_state - observed_state) ** 2)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer_online.step()\n",
    "        \n",
    "        # Track best solution\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_pred_state = next_state.detach().clone()\n",
    "    \n",
    "    # Update current state and time with optimal prediction\n",
    "    current_state = best_pred_state.detach().requires_grad_(True)\n",
    "    current_time = target_time\n",
    "    \n",
    "    # Store results\n",
    "    online_trajectories.append((obs_idx, current_state.detach().clone()))\n",
    "    online_losses.append((obs_idx, best_loss))\n",
    "    online_params.append({\n",
    "        'obs_idx': obs_idx,\n",
    "        'sigma': sigma_online.item(),\n",
    "        'rho': rho_online.item(),\n",
    "        'beta': beta_online.item(),\n",
    "        'loss': best_loss\n",
    "    })\n",
    "    \n",
    "    if obs_idx % 1 == 0 or obs_idx == len(true_traj) - 1:\n",
    "        print(f\"Obs {obs_idx:3d} | Loss={best_loss:.6f} | \"\n",
    "              f\"sigma={sigma_online.item():.3f}, rho={rho_online.item():.3f}, beta={beta_online.item():.3f}\")\n",
    "\n",
    "# Compute final loss from all concatenated online trajectories\n",
    "final_online_traj = torch.stack([traj for _, traj in online_trajectories])\n",
    "final_loss = torch.mean((final_online_traj - true_states) ** 2).item()\n",
    "print(f\"Final online estimation loss: {final_loss:.6f}\")\n",
    "\n",
    "print(f\"\\nOnline estimation complete. Processed {len(true_traj)-1} observations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3b50c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trajectories\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "# 3D plot of all trajectories\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "\n",
    "# Plot the true (observed) trajectory\n",
    "true_traj_np = true_traj.detach().numpy()\n",
    "ax1.plot(true_traj_np[:, 0], true_traj_np[:, 1], true_traj_np[:, 2], \n",
    "         'o-', linewidth=2, markersize=4, label='Observed', color='black')\n",
    "final_online_traj_np = final_online_traj.detach().numpy()\n",
    "ax1.plot(final_online_traj_np[:, 0], final_online_traj_np[:, 1], final_online_traj_np[:, 2], \n",
    "         'o-', linewidth=2, markersize=4, label='Estimated', color='red')\n",
    "\n",
    "\n",
    "ax1.set_xlabel('X')\n",
    "ax1.set_ylabel('Y')\n",
    "ax1.set_zlabel('Z')\n",
    "ax1.set_title('Lorenz Trajectories Over Optimization')\n",
    "ax1.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2dee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_observations(state0):\n",
    "    # Fake \"observed\" data (normally you'd load real data)\n",
    "    with torch.no_grad():\n",
    "        true_time, true_traj = run_lorenz(\n",
    "            state0,\n",
    "            t0=0,\n",
    "            steps=100,\n",
    "            dt=0.01,\n",
    "            sigma=torch.tensor(10.0),\n",
    "            rho=torch.tensor(28.0),\n",
    "            beta=torch.tensor(8/3),\n",
    "        )\n",
    "        return true_time, true_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d58ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_optimization(state0, num_tot_steps=200000):\n",
    "    # Reset parameters for online estimation\n",
    "    sigma_online = torch.tensor(8.0, requires_grad=True)\n",
    "    rho_online   = torch.tensor(20.0, requires_grad=True)\n",
    "    beta_online  = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "    true_time, true_traj = get_observations(state0)\n",
    "\n",
    "    # Starting state\n",
    "    current_state = state0.clone()\n",
    "    current_time = 0.0\n",
    "\n",
    "    # Extract times and states from true trajectory\n",
    "    true_times = true_time\n",
    "    true_states = true_traj\n",
    "\n",
    "    # Store results for visualization\n",
    "    online_trajectories = []\n",
    "    online_losses = []\n",
    "    online_params = []\n",
    "\n",
    "    # Number of optimization trials per observation\n",
    "    # In the previous optimization we ran 2000 trials, each 100 time steps\n",
    "    # Let's keep the total number of simulated steps the same such that per \n",
    "    # time step we optimize with n_trials_online trials\n",
    "    n_trials_online = int(num_tot_steps / 100)\n",
    "\n",
    "    # Process each observation\n",
    "    for obs_idx in range(len(true_states)):\n",
    "        target_time = true_times[obs_idx].item()\n",
    "        dt_to_target = target_time - current_time\n",
    "        num_steps_to_target = 1 #int(dt_to_target / 0.01)  # Currently one step from previous observation to next\n",
    "        observed_state = true_states[obs_idx]\n",
    "        target_time = num_steps_to_target * 0.01 + current_time\n",
    "        # Optimizer for this online step\n",
    "        optimizer_online = torch.optim.Adam([sigma_online, rho_online, beta_online], lr=1e-2)\n",
    "        \n",
    "        best_loss = float('inf')\n",
    "        best_pred_state = None\n",
    "        \n",
    "        # Run N optimization trials for this observation\n",
    "        for trial in range(n_trials_online):\n",
    "            optimizer_online.zero_grad()\n",
    "            \n",
    "            # Advance from current state to next observation point\n",
    "            next_state = run_lorenz(current_state, current_time, num_steps_to_target, 0.01, sigma_online, rho_online, beta_online)[1][-1]\n",
    "            \n",
    "            # Compute loss against observed state\n",
    "            loss = torch.mean((next_state - observed_state) ** 2)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer_online.step()\n",
    "            \n",
    "            # Track best solution\n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss.item()\n",
    "                best_pred_state = next_state.detach().clone()\n",
    "        \n",
    "        # Update current state and time with optimal prediction\n",
    "        current_state = best_pred_state.detach().requires_grad_(True)\n",
    "        current_time = target_time\n",
    "        \n",
    "        # Store results\n",
    "        online_trajectories.append((obs_idx, current_state.detach().clone()))\n",
    "        online_losses.append((obs_idx, best_loss))\n",
    "        online_params.append({\n",
    "            'obs_idx': obs_idx,\n",
    "            'sigma': sigma_online.item(),\n",
    "            'rho': rho_online.item(),\n",
    "            'beta': beta_online.item(),\n",
    "            'loss': best_loss\n",
    "        })\n",
    "        \n",
    "        if obs_idx % 1 == 0 or obs_idx == len(true_traj) - 1:\n",
    "            print(f\"Obs {obs_idx:3d} | Loss={best_loss:.6f} | \"\n",
    "                f\"sigma={sigma_online.item():.3f}, rho={rho_online.item():.3f}, beta={beta_online.item():.3f}\")\n",
    "\n",
    "    # Compute final loss from all concatenated online trajectories\n",
    "    final_online_traj = torch.stack([traj for _, traj in online_trajectories])\n",
    "    final_loss = torch.mean((final_online_traj - true_states) ** 2).item()\n",
    "    print(f\"Final online estimation loss: {final_loss:.6f}\")\n",
    "\n",
    "    print(f\"\\nOnline estimation complete. Processed {len(true_traj)-1} observations.\")\n",
    "    return final_online_traj, final_loss\n",
    "\n",
    "def scenario_optimization(state0, num_tot_steps=200000):\n",
    "    # Set number of trials \n",
    "    num_trials = int(num_tot_steps / 100)\n",
    "\n",
    "\n",
    "    # Initial parameter choices\n",
    "    sigma = torch.tensor(8.0, requires_grad=True)\n",
    "    rho   = torch.tensor(20.0, requires_grad=True)\n",
    "    beta  = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "    true_time, true_traj = get_observations(state0)\n",
    "\n",
    "    optimizer = torch.optim.Adam([sigma, rho, beta], lr=1e-2)\n",
    "\n",
    "    # Store trajectories and losses for visualization\n",
    "    stored_trajectories = []\n",
    "    stored_losses = []\n",
    "\n",
    "    for trial in range(num_trials):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred_time, pred_traj = run_lorenz(state0, t0=0, steps=100, dt=0.01, sigma=sigma, rho=rho, beta=beta)\n",
    "        loss = torch.mean((pred_traj - true_traj) ** 2)\n",
    "\n",
    "        loss.backward()   # <-- AUTOGRAD HERE\n",
    "        optimizer.step()\n",
    "\n",
    "        if trial % 20 == 0 or trial == num_trials - 1:\n",
    "            print(f\"Trial {trial:4d} | Loss={loss.item():.6f} \"\n",
    "                f\"sigma={sigma.item():.3f}, rho={rho.item():.3f}, beta={beta.item():.3f}\")\n",
    "            # Store trajectory and loss for plotting\n",
    "            stored_trajectories.append((trial, pred_traj.detach().clone()))\n",
    "            stored_losses.append((trial, loss.item()))\n",
    "    return [traj for _, traj in stored_trajectories], [loss for _, loss in stored_losses]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c5f58c",
   "metadata": {},
   "source": [
    "Now let's compare the two methods, the scenario optimization and the online-optimization. We can use the same number of total time steps used during the optimization and compute the resulting loss from both methods. Let's run this and then plot the loss as a function of the total number of time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa75df16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial condition\n",
    "state0 = torch.tensor([1.0, 1.0, 1.0])\n",
    "output = []\n",
    "for num_tot_steps in [int(x) for x in np.logspace(np.log10(2000), np.log10(200000), 10)]:\n",
    "    output.append((num_tot_steps, scenario_optimization(state0, num_tot_steps=num_tot_steps), online_optimization(state0, num_tot_steps=num_tot_steps)))\n",
    "# Extract data for plotting\n",
    "steps = [item[0] for item in output]\n",
    "scenario_losses = [item[1] for item in output]\n",
    "online_losses = [item[2] for item in output]\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(steps, scenario_losses, 'b-o', label='Scenario Optimization', linewidth=2, markersize=6)\n",
    "plt.plot(steps, online_losses, 'r-s', label='Online Optimization', linewidth=2, markersize=6)\n",
    "plt.xlabel('Number of Total Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Comparison of Optimization Methods vs Total Steps')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fa0066",
   "metadata": {},
   "source": [
    "We can see that the online optimization, which optimizes each timestep, performs better as soon as the number of steps is reasonably large. For very large numbers the two approaches become identical. The way I interpret this is that the online optimization per timestep optimizes smaller problems with less complicated loss functions. Therefore, the overall number of steps needed to optimize the full trajectory ends up being smaller than for the scenario optimization approach that optimizes the full trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f116906f",
   "metadata": {},
   "source": [
    "The real-time online optimization of course only makes sense if the optimization between observations can get carried out faster than real-time. That is, as a new observation comes in, we need to advance the state from the time the last observation was obtained, but not once, but N-trial times."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
